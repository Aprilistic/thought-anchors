{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VA Attention Analysis (Problem 9, Qwen3-4B)\n",
        "\n",
        "Goal: for the 10 exported rollout traces `problem_9_va_00..09`, compute VA-related attention numbers, pick the most distinctive trace, and plot early/mid/late-layer attention patterns with VA sentences highlighted.\n",
        "\n",
        "Notes:\n",
        "- This notebook uses existing whitebox utilities under `whitebox-analyses/attention_analysis/`.\n",
        "- Attention matrices are cached to disk (default: `attn_cache/`). First run will be expensive (model forward pass), subsequent runs are fast.\n",
        "- We intentionally do NOT plot all layers. We pick representative early/mid/late layers, and within each layer select the most VA-distinct head for the chosen rollout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef223002",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'whitebox-analyses').exists():\n",
        "            return p\n",
        "    raise RuntimeError(f'Could not find repo root from {start}')\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "WHITEBOX_ROOT = (REPO_ROOT / 'whitebox-analyses').resolve()\n",
        "sys.path.insert(0, str(WHITEBOX_ROOT))\n",
        "print('REPO_ROOT', REPO_ROOT)\n",
        "print('WHITEBOX_ROOT', WHITEBOX_ROOT)\n",
        "\n",
        "from pytorch_models.model_config import model2layers_heads\n",
        "from attention_analysis.attn_funcs import get_avg_attention_matrix, get_vertical_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Config ---\n",
        "MODEL_NAME = 'qwen3-4b'\n",
        "CI = 'correct_base_solution'\n",
        "\n",
        "VA_ROOT = REPO_ROOT / 'rollouts' / 'Qwen3-4B-Thinking-2507' / 'temperature_0.6_top_p_0.95' / 'va_examples'\n",
        "PROBLEM_PREFIX = 'problem_9_va_'\n",
        "\n",
        "# Cache dir for sentence-averaged attention matrices.\n",
        "# Keep this separate if you don't want to mix with other experiments.\n",
        "ATTN_CACHE_DIR = 'attn_cache'\n",
        "\n",
        "# Representative layers (early / mid / late) for Qwen3-4B (36 layers).\n",
        "N_LAYERS, N_HEADS = model2layers_heads(MODEL_NAME)\n",
        "LAYERS = [max(0, int(0.10 * (N_LAYERS - 1))), int(0.50 * (N_LAYERS - 1)), int(0.90 * (N_LAYERS - 1))]\n",
        "LAYERS = sorted(set(LAYERS))\n",
        "\n",
        "print('MODEL_NAME', MODEL_NAME)\n",
        "print('N_LAYERS, N_HEADS', (N_LAYERS, N_HEADS))\n",
        "print('Representative layers', LAYERS)\n",
        "print('VA_ROOT exists', VA_ROOT.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discover the 10 rollout examples for problem 9\n",
        "ci_dir = VA_ROOT / CI\n",
        "rollout_dirs = sorted([p for p in ci_dir.iterdir() if p.is_dir() and p.name.startswith(PROBLEM_PREFIX)])\n",
        "print('Found', len(rollout_dirs), 'rollout dirs')\n",
        "for p in rollout_dirs:\n",
        "    print('-', p.name)\n",
        "\n",
        "assert len(rollout_dirs) > 0, f'No {PROBLEM_PREFIX} dirs under {ci_dir}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions\n",
        "We compute a per-(rollout, layer, head) distinctiveness score based on vertical attention scores:\n",
        "- `vertical_scores = get_vertical_scores(avg_attention_matrix)`\n",
        "- `delta = mean(vertical_scores[VA]) - mean(vertical_scores[nonVA])`\n",
        "\n",
        "Then we pick, for each layer, the head with max `abs(delta)` and aggregate across 3 layers to pick the most distinctive rollout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_chunks_and_va_mask(rollout_dir: Path):\n",
        "    chunks_path = rollout_dir / 'chunks.json'\n",
        "    labeled_path = rollout_dir / 'chunks_labeled.json'\n",
        "\n",
        "    chunks_obj = json.loads(chunks_path.read_text(encoding='utf-8'))\n",
        "    chunks = chunks_obj.get('chunks', [])\n",
        "    labeled = json.loads(labeled_path.read_text(encoding='utf-8'))\n",
        "\n",
        "    if not isinstance(chunks, list):\n",
        "        chunks = []\n",
        "    if not isinstance(labeled, list):\n",
        "        labeled = []\n",
        "\n",
        "    # We treat 'chunks' as the sentence units for attention boundaries to avoid mismatch warnings\n",
        "    # from base_solution['full_cot'] and to guarantee alignment with chunks_labeled.json.\n",
        "    n = len(chunks)\n",
        "    labeled = labeled[:n]\n",
        "\n",
        "    va_mask = np.zeros((n,), dtype=bool)\n",
        "    for i, c in enumerate(labeled):\n",
        "        tags = c.get('function_tags', []) if isinstance(c, dict) else []\n",
        "        if isinstance(tags, list) and any(t == 'verbalized_evaluation_awareness' for t in tags):\n",
        "            va_mask[i] = True\n",
        "\n",
        "    text = '\\n'.join(str(x) for x in chunks)\n",
        "    sentences = [str(x) for x in chunks]\n",
        "    return text, sentences, va_mask, labeled\n",
        "\n",
        "def safe_mean(arr: np.ndarray) -> float:\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "    if arr.size == 0:\n",
        "        return float('nan')\n",
        "    return float(np.nanmean(arr))\n",
        "\n",
        "def compute_va_stats_for_head(avg_mat: np.ndarray, va_mask: np.ndarray, proximity_ignore: int = 4):\n",
        "    n = avg_mat.shape[0]\n",
        "    assert va_mask.shape[0] == n\n",
        "    non_mask = ~va_mask\n",
        "\n",
        "    vert = get_vertical_scores(avg_mat, proximity_ignore=proximity_ignore, control_depth=True, score_type='mean')\n",
        "    va_vert = vert[va_mask]\n",
        "    non_vert = vert[non_mask]\n",
        "\n",
        "    mean_va_vert = safe_mean(va_vert)\n",
        "    mean_non_vert = safe_mean(non_vert)\n",
        "    delta_vert = mean_va_vert - mean_non_vert\n",
        "\n",
        "    # Simple matrix-based aggregates (incoming/outgoing attention).\n",
        "    # These are not causal; they are descriptive summaries.\n",
        "    mean_in_va = safe_mean(avg_mat[non_mask][:, va_mask])\n",
        "    mean_in_non = safe_mean(avg_mat[non_mask][:, non_mask])\n",
        "    mean_out_va = safe_mean(avg_mat[va_mask][:, non_mask])\n",
        "    mean_va_to_va = safe_mean(avg_mat[va_mask][:, va_mask])\n",
        "\n",
        "    lift_in = float('nan')\n",
        "    if np.isfinite(mean_in_va) and np.isfinite(mean_in_non) and mean_in_non != 0:\n",
        "        lift_in = mean_in_va / mean_in_non\n",
        "\n",
        "    return {\n",
        "        'mean_va_vert': mean_va_vert,\n",
        "        'mean_nonva_vert': mean_non_vert,\n",
        "        'delta_va_vert': delta_vert,\n",
        "        'mean_in_va': mean_in_va,\n",
        "        'mean_in_nonva': mean_in_non,\n",
        "        'lift_in_va': lift_in,\n",
        "        'mean_out_va': mean_out_va,\n",
        "        'mean_va_to_va': mean_va_to_va,\n",
        "        'vert_scores': vert,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: compute distinctiveness numbers across 10 rollouts\n",
        "This will trigger attention caching per rollout (expensive on first run).\n",
        "\n",
        "If you want to do a quick dry run (no model inference), set `RUN_ATTENTION = False` to only inspect VA counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_ATTENTION = True\n",
        "PROXIMITY_IGNORE = 4\n",
        "HEADS_TO_SCAN = list(range(N_HEADS))\n",
        "\n",
        "rows = []\n",
        "\n",
        "for rollout_dir in rollout_dirs:\n",
        "    rollout_id = rollout_dir.name\n",
        "    text, sentences, va_mask, labeled = load_chunks_and_va_mask(rollout_dir)\n",
        "    n_sent = len(sentences)\n",
        "    n_va = int(va_mask.sum())\n",
        "\n",
        "    if not RUN_ATTENTION:\n",
        "        rows.append({\n",
        "            'rollout_id': rollout_id,\n",
        "            'n_sentences': n_sent,\n",
        "            'n_va': n_va,\n",
        "            'va_rate': (n_va / n_sent) if n_sent else float('nan'),\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # First call triggers cache for all layers/heads for this text_id (fast afterwards).\n",
        "    _ = get_avg_attention_matrix(\n",
        "        text=text,\n",
        "        model_name=MODEL_NAME,\n",
        "        layer=LAYERS[0],\n",
        "        head=0,\n",
        "        sentences=sentences,\n",
        "        cache_dir=ATTN_CACHE_DIR,\n",
        "        force_recompute=False,\n",
        "    )\n",
        "\n",
        "    for layer in LAYERS:\n",
        "        for head in HEADS_TO_SCAN:\n",
        "            avg_mat = get_avg_attention_matrix(\n",
        "                text=text,\n",
        "                model_name=MODEL_NAME,\n",
        "                layer=layer,\n",
        "                head=head,\n",
        "                sentences=sentences,\n",
        "                cache_dir=ATTN_CACHE_DIR,\n",
        "                force_recompute=False,\n",
        "            )\n",
        "\n",
        "            # Keep consistent with plot_one_attn_matrix.py which trims prompt/output bins in legacy runs.\n",
        "            if avg_mat.shape[0] >= 3:\n",
        "                avg_mat2 = avg_mat[1:-1, 1:-1]\n",
        "                va_mask2 = va_mask[1:-1]\n",
        "            else:\n",
        "                avg_mat2 = avg_mat\n",
        "                va_mask2 = va_mask\n",
        "\n",
        "            stats = compute_va_stats_for_head(avg_mat2, va_mask2, proximity_ignore=PROXIMITY_IGNORE)\n",
        "\n",
        "            rows.append({\n",
        "                'rollout_id': rollout_id,\n",
        "                'layer': int(layer),\n",
        "                'head': int(head),\n",
        "                'n_sentences': int(avg_mat2.shape[0]),\n",
        "                'n_va': int(va_mask2.sum()),\n",
        "                'va_rate': float(va_mask2.sum() / max(1, avg_mat2.shape[0])),\n",
        "                'delta_va_vert': stats['delta_va_vert'],\n",
        "                'mean_va_vert': stats['mean_va_vert'],\n",
        "                'mean_nonva_vert': stats['mean_nonva_vert'],\n",
        "                'lift_in_va': stats['lift_in_va'],\n",
        "                'mean_in_va': stats['mean_in_va'],\n",
        "                'mean_in_nonva': stats['mean_in_nonva'],\n",
        "                'mean_out_va': stats['mean_out_va'],\n",
        "                'mean_va_to_va': stats['mean_va_to_va'],\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not RUN_ATTENTION:\n",
        "    display(df.sort_values(['n_va', 'n_sentences'], ascending=[False, False]))\n",
        "else:\n",
        "    # Pick the most VA-distinct head per layer per rollout (max abs delta).\n",
        "    df['abs_delta_va_vert'] = df['delta_va_vert'].abs()\n",
        "    best_per_layer = (\n",
        "        df.sort_values('abs_delta_va_vert', ascending=False)\n",
        "          .groupby(['rollout_id', 'layer'], as_index=False)\n",
        "          .first()\n",
        "    )\n",
        "\n",
        "    # Aggregate distinctiveness across the representative layers.\n",
        "    agg = (\n",
        "        best_per_layer.groupby('rollout_id')\n",
        "        .agg({\n",
        "            'abs_delta_va_vert': 'sum',\n",
        "            'n_va': 'first',\n",
        "            'n_sentences': 'first',\n",
        "        })\n",
        "        .rename(columns={'abs_delta_va_vert': 'distinctiveness_score'})\n",
        "        .reset_index()\n",
        "    )\n",
        "    agg['va_rate'] = agg['n_va'] / agg['n_sentences'].clip(lower=1)\n",
        "\n",
        "    display(agg.sort_values('distinctiveness_score', ascending=False))\n",
        "\n",
        "    best_rollout_id = agg.sort_values('distinctiveness_score', ascending=False).iloc[0]['rollout_id']\n",
        "    print('Best rollout by distinctiveness:', best_rollout_id)\n",
        "\n",
        "    best_heads = best_per_layer[best_per_layer['rollout_id'] == best_rollout_id].sort_values('layer')\n",
        "    display(best_heads[['layer', 'head', 'delta_va_vert', 'lift_in_va', 'n_va', 'n_sentences']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Plot early/mid/late attention matrices for the most distinctive rollout\n",
        "We replicate the core idea of `plot_one_attn_matrix.py` but inline here so we can target `problem_9_va_XX` directories directly and produce multiple plots in one notebook cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def overlay_va_highlights(ax, va_mask, axis='both', color='red', alpha=0.18):\n",
        "    axis = (axis or 'both').lower()\n",
        "    for idx, is_hit in enumerate(va_mask):\n",
        "        if not is_hit:\n",
        "            continue\n",
        "        if axis in ('rows', 'both'):\n",
        "            ax.axhspan(idx - 0.5, idx + 0.5, color=color, alpha=alpha, linewidth=0)\n",
        "        if axis in ('cols', 'both'):\n",
        "            ax.axvspan(idx - 0.5, idx + 0.5, color=color, alpha=alpha, linewidth=0)\n",
        "\n",
        "def plot_attn_heatmap(avg_mat, va_mask, title):\n",
        "    # Use a robust vmax based on lower triangle to avoid being dominated by outliers.\n",
        "    tril = np.tril(avg_mat)\n",
        "    vmax = float(np.nanquantile(tril, 0.99)) if np.isfinite(tril).any() else 1.0\n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    im = ax.imshow(avg_mat, vmin=0, vmax=vmax, cmap=plt.cm.Blues)\n",
        "    overlay_va_highlights(ax, va_mask, axis='both', color='red', alpha=0.18)\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_xlabel('Sentence position')\n",
        "    ax.set_ylabel('Sentence position')\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_vertical_scores(vert_scores, va_mask, title):\n",
        "    x = np.arange(len(vert_scores))\n",
        "    fig, ax = plt.subplots(figsize=(10, 2.5))\n",
        "    ax.plot(x, vert_scores, color='#0f172a', linewidth=1)\n",
        "    ax.scatter(x[~va_mask], np.asarray(vert_scores)[~va_mask], s=12, color='#94a3b8', alpha=0.8, label='non-VA')\n",
        "    ax.scatter(x[va_mask], np.asarray(vert_scores)[va_mask], s=24, color='#ef4444', alpha=0.95, label='VA')\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_xlabel('Sentence idx')\n",
        "    ax.set_ylabel('Vertical score')\n",
        "    ax.legend(loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert RUN_ATTENTION, 'Set RUN_ATTENTION=True to generate attention plots.'\n",
        "\n",
        "best_dir = ci_dir / best_rollout_id\n",
        "text, sentences, va_mask, labeled = load_chunks_and_va_mask(best_dir)\n",
        "\n",
        "# Print VA indices for this rollout\n",
        "va_idxs = [i for i, v in enumerate(va_mask.tolist()) if v]\n",
        "print('Best rollout:', best_rollout_id)\n",
        "print('n_sentences:', len(sentences), 'n_va:', len(va_idxs))\n",
        "print('va_idxs:', va_idxs)\n",
        "\n",
        "for _, row in best_heads.iterrows():\n",
        "    layer = int(row['layer'])\n",
        "    head = int(row['head'])\n",
        "\n",
        "    avg_mat = get_avg_attention_matrix(\n",
        "        text=text,\n",
        "        model_name=MODEL_NAME,\n",
        "        layer=layer,\n",
        "        head=head,\n",
        "        sentences=sentences,\n",
        "        cache_dir=ATTN_CACHE_DIR,\n",
        "        force_recompute=False,\n",
        "    )\n",
        "\n",
        "    if avg_mat.shape[0] >= 3:\n",
        "        avg_mat2 = avg_mat[1:-1, 1:-1]\n",
        "        va_mask2 = va_mask[1:-1]\n",
        "    else:\n",
        "        avg_mat2 = avg_mat\n",
        "        va_mask2 = va_mask\n",
        "\n",
        "    stats = compute_va_stats_for_head(avg_mat2, va_mask2, proximity_ignore=PROXIMITY_IGNORE)\n",
        "\n",
        "    title = f\"{best_rollout_id} | layer={layer} head={head} | delta_va_vert={stats['delta_va_vert']:.4f} lift_in_va={stats['lift_in_va']:.3f}\"\n",
        "    plot_attn_heatmap(avg_mat2, va_mask2, title)\n",
        "    plot_vertical_scores(stats['vert_scores'], va_mask2, f\"Vertical scores | {best_rollout_id} | layer={layer} head={head}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
